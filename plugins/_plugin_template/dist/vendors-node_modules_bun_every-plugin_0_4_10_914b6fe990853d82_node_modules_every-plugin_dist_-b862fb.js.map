{"version":3,"file":"vendors-node_modules_bun_every-plugin_0_4_10_914b6fe990853d82_node_modules_every-plugin_dist_-b862fb.js","sources":["webpack://data-provider_template/../../node_modules/.bun/@orpc+client@1.10.4/node_modules/@orpc/client/dist/adapters/standard/index.mjs","webpack://data-provider_template/../../node_modules/.bun/@orpc+client@1.10.4/node_modules/@orpc/client/dist/shared/client.Di65MWxv.mjs","webpack://data-provider_template/../../node_modules/.bun/@orpc+experimental-publisher@1.10.4/node_modules/@orpc/experimental-publisher/dist/adapters/ioredis.mjs","webpack://data-provider_template/../../node_modules/.bun/@orpc+experimental-publisher@1.10.4/node_modules/@orpc/experimental-publisher/dist/adapters/memory.mjs","webpack://data-provider_template/../../node_modules/.bun/@orpc+experimental-publisher@1.10.4/node_modules/@orpc/experimental-publisher/dist/adapters/upstash-redis.mjs","webpack://data-provider_template/../../node_modules/.bun/@orpc+experimental-publisher@1.10.4/node_modules/@orpc/experimental-publisher/dist/index.mjs","webpack://data-provider_template/../../node_modules/.bun/@orpc+experimental-publisher@1.10.4/node_modules/@orpc/experimental-publisher/dist/shared/experimental-publisher.BtlOkhPO.mjs","webpack://data-provider_template/../../node_modules/.bun/@orpc+standard-server-fetch@1.10.4/node_modules/@orpc/standard-server-fetch/dist/index.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.10+914b6fe990853d82/node_modules/every-plugin/dist/chunk-QQAxy9l9.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.10+914b6fe990853d82/node_modules/every-plugin/dist/effect.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.10+914b6fe990853d82/node_modules/every-plugin/dist/zod-BVo51kgJ.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.10+914b6fe990853d82/node_modules/every-plugin/dist/zod.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.9+914b6fe990853d82/node_modules/every-plugin/dist/chunk-QQAxy9l9.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.9+914b6fe990853d82/node_modules/every-plugin/dist/orpc.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.9+914b6fe990853d82/node_modules/every-plugin/dist/zod-BVo51kgJ.mjs","webpack://data-provider_template/../../node_modules/.bun/every-plugin@0.4.9+914b6fe990853d82/node_modules/every-plugin/dist/zod.mjs"],"sourcesContent":["export { C as CompositeStandardLinkPlugin, a as STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES, S as StandardLink, b as StandardRPCJsonSerializer, c as StandardRPCLink, d as StandardRPCLinkCodec, e as StandardRPCSerializer, g as getMalformedResponseErrorCode, t as toHttpPath, f as toStandardHeaders } from '../../shared/client.Di65MWxv.mjs';\nimport '@orpc/shared';\nimport '@orpc/standard-server';\nimport '../../shared/client.DmkMd_GB.mjs';\nimport '@orpc/standard-server-fetch';\n","import { toArray, runWithSpan, ORPC_NAME, isAsyncIteratorObject, asyncIteratorWithSpan, intercept, getGlobalOtelConfig, isObject, value, stringifyJSON } from '@orpc/shared';\nimport { mergeStandardHeaders, ErrorEvent } from '@orpc/standard-server';\nimport { C as COMMON_ORPC_ERROR_DEFS, d as isORPCErrorStatus, e as isORPCErrorJson, g as createORPCErrorFromJson, c as ORPCError, m as mapEventIterator, t as toORPCError } from './client.DmkMd_GB.mjs';\nimport { toStandardHeaders as toStandardHeaders$1 } from '@orpc/standard-server-fetch';\n\nclass CompositeStandardLinkPlugin {\n  plugins;\n  constructor(plugins = []) {\n    this.plugins = [...plugins].sort((a, b) => (a.order ?? 0) - (b.order ?? 0));\n  }\n  init(options) {\n    for (const plugin of this.plugins) {\n      plugin.init?.(options);\n    }\n  }\n}\n\nclass StandardLink {\n  constructor(codec, sender, options = {}) {\n    this.codec = codec;\n    this.sender = sender;\n    const plugin = new CompositeStandardLinkPlugin(options.plugins);\n    plugin.init(options);\n    this.interceptors = toArray(options.interceptors);\n    this.clientInterceptors = toArray(options.clientInterceptors);\n  }\n  interceptors;\n  clientInterceptors;\n  call(path, input, options) {\n    return runWithSpan(\n      { name: `${ORPC_NAME}.${path.join(\"/\")}`, signal: options.signal },\n      (span) => {\n        span?.setAttribute(\"rpc.system\", ORPC_NAME);\n        span?.setAttribute(\"rpc.method\", path.join(\".\"));\n        if (isAsyncIteratorObject(input)) {\n          input = asyncIteratorWithSpan(\n            { name: \"consume_event_iterator_input\", signal: options.signal },\n            input\n          );\n        }\n        return intercept(this.interceptors, { ...options, path, input }, async ({ path: path2, input: input2, ...options2 }) => {\n          const otelConfig = getGlobalOtelConfig();\n          let otelContext;\n          const currentSpan = otelConfig?.trace.getActiveSpan() ?? span;\n          if (currentSpan && otelConfig) {\n            otelContext = otelConfig?.trace.setSpan(otelConfig.context.active(), currentSpan);\n          }\n          const request = await runWithSpan(\n            { name: \"encode_request\", context: otelContext },\n            () => this.codec.encode(path2, input2, options2)\n          );\n          const response = await intercept(\n            this.clientInterceptors,\n            { ...options2, input: input2, path: path2, request },\n            ({ input: input3, path: path3, request: request2, ...options3 }) => {\n              return runWithSpan(\n                { name: \"send_request\", signal: options3.signal, context: otelContext },\n                () => this.sender.call(request2, options3, path3, input3)\n              );\n            }\n          );\n          const output = await runWithSpan(\n            { name: \"decode_response\", context: otelContext },\n            () => this.codec.decode(response, options2, path2, input2)\n          );\n          if (isAsyncIteratorObject(output)) {\n            return asyncIteratorWithSpan(\n              { name: \"consume_event_iterator_output\", signal: options2.signal },\n              output\n            );\n          }\n          return output;\n        });\n      }\n    );\n  }\n}\n\nconst STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES = {\n  BIGINT: 0,\n  DATE: 1,\n  NAN: 2,\n  UNDEFINED: 3,\n  URL: 4,\n  REGEXP: 5,\n  SET: 6,\n  MAP: 7\n};\nclass StandardRPCJsonSerializer {\n  customSerializers;\n  constructor(options = {}) {\n    this.customSerializers = options.customJsonSerializers ?? [];\n    if (this.customSerializers.length !== new Set(this.customSerializers.map((custom) => custom.type)).size) {\n      throw new Error(\"Custom serializer type must be unique.\");\n    }\n  }\n  serialize(data, segments = [], meta = [], maps = [], blobs = []) {\n    for (const custom of this.customSerializers) {\n      if (custom.condition(data)) {\n        const result = this.serialize(custom.serialize(data), segments, meta, maps, blobs);\n        meta.push([custom.type, ...segments]);\n        return result;\n      }\n    }\n    if (data instanceof Blob) {\n      maps.push(segments);\n      blobs.push(data);\n      return [data, meta, maps, blobs];\n    }\n    if (typeof data === \"bigint\") {\n      meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.BIGINT, ...segments]);\n      return [data.toString(), meta, maps, blobs];\n    }\n    if (data instanceof Date) {\n      meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.DATE, ...segments]);\n      if (Number.isNaN(data.getTime())) {\n        return [null, meta, maps, blobs];\n      }\n      return [data.toISOString(), meta, maps, blobs];\n    }\n    if (Number.isNaN(data)) {\n      meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.NAN, ...segments]);\n      return [null, meta, maps, blobs];\n    }\n    if (data instanceof URL) {\n      meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.URL, ...segments]);\n      return [data.toString(), meta, maps, blobs];\n    }\n    if (data instanceof RegExp) {\n      meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.REGEXP, ...segments]);\n      return [data.toString(), meta, maps, blobs];\n    }\n    if (data instanceof Set) {\n      const result = this.serialize(Array.from(data), segments, meta, maps, blobs);\n      meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.SET, ...segments]);\n      return result;\n    }\n    if (data instanceof Map) {\n      const result = this.serialize(Array.from(data.entries()), segments, meta, maps, blobs);\n      meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.MAP, ...segments]);\n      return result;\n    }\n    if (Array.isArray(data)) {\n      const json = data.map((v, i) => {\n        if (v === void 0) {\n          meta.push([STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.UNDEFINED, ...segments, i]);\n          return v;\n        }\n        return this.serialize(v, [...segments, i], meta, maps, blobs)[0];\n      });\n      return [json, meta, maps, blobs];\n    }\n    if (isObject(data)) {\n      const json = {};\n      for (const k in data) {\n        if (k === \"toJSON\" && typeof data[k] === \"function\") {\n          continue;\n        }\n        json[k] = this.serialize(data[k], [...segments, k], meta, maps, blobs)[0];\n      }\n      return [json, meta, maps, blobs];\n    }\n    return [data, meta, maps, blobs];\n  }\n  deserialize(json, meta, maps, getBlob) {\n    const ref = { data: json };\n    if (maps && getBlob) {\n      maps.forEach((segments, i) => {\n        let currentRef = ref;\n        let preSegment = \"data\";\n        segments.forEach((segment) => {\n          currentRef = currentRef[preSegment];\n          preSegment = segment;\n        });\n        currentRef[preSegment] = getBlob(i);\n      });\n    }\n    for (const item of meta) {\n      const type = item[0];\n      let currentRef = ref;\n      let preSegment = \"data\";\n      for (let i = 1; i < item.length; i++) {\n        currentRef = currentRef[preSegment];\n        preSegment = item[i];\n      }\n      for (const custom of this.customSerializers) {\n        if (custom.type === type) {\n          currentRef[preSegment] = custom.deserialize(currentRef[preSegment]);\n          break;\n        }\n      }\n      switch (type) {\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.BIGINT:\n          currentRef[preSegment] = BigInt(currentRef[preSegment]);\n          break;\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.DATE:\n          currentRef[preSegment] = new Date(currentRef[preSegment] ?? \"Invalid Date\");\n          break;\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.NAN:\n          currentRef[preSegment] = Number.NaN;\n          break;\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.UNDEFINED:\n          currentRef[preSegment] = void 0;\n          break;\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.URL:\n          currentRef[preSegment] = new URL(currentRef[preSegment]);\n          break;\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.REGEXP: {\n          const [, pattern, flags] = currentRef[preSegment].match(/^\\/(.*)\\/([a-z]*)$/);\n          currentRef[preSegment] = new RegExp(pattern, flags);\n          break;\n        }\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.SET:\n          currentRef[preSegment] = new Set(currentRef[preSegment]);\n          break;\n        case STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES.MAP:\n          currentRef[preSegment] = new Map(currentRef[preSegment]);\n          break;\n      }\n    }\n    return ref.data;\n  }\n}\n\nfunction toHttpPath(path) {\n  return `/${path.map(encodeURIComponent).join(\"/\")}`;\n}\nfunction toStandardHeaders(headers) {\n  if (typeof headers.forEach === \"function\") {\n    return toStandardHeaders$1(headers);\n  }\n  return headers;\n}\nfunction getMalformedResponseErrorCode(status) {\n  return Object.entries(COMMON_ORPC_ERROR_DEFS).find(([, def]) => def.status === status)?.[0] ?? \"MALFORMED_ORPC_ERROR_RESPONSE\";\n}\n\nclass StandardRPCLinkCodec {\n  constructor(serializer, options) {\n    this.serializer = serializer;\n    this.baseUrl = options.url;\n    this.maxUrlLength = options.maxUrlLength ?? 2083;\n    this.fallbackMethod = options.fallbackMethod ?? \"POST\";\n    this.expectedMethod = options.method ?? this.fallbackMethod;\n    this.headers = options.headers ?? {};\n  }\n  baseUrl;\n  maxUrlLength;\n  fallbackMethod;\n  expectedMethod;\n  headers;\n  async encode(path, input, options) {\n    let headers = toStandardHeaders(await value(this.headers, options, path, input));\n    if (options.lastEventId !== void 0) {\n      headers = mergeStandardHeaders(headers, { \"last-event-id\": options.lastEventId });\n    }\n    const expectedMethod = await value(this.expectedMethod, options, path, input);\n    const baseUrl = await value(this.baseUrl, options, path, input);\n    const url = new URL(baseUrl);\n    url.pathname = `${url.pathname.replace(/\\/$/, \"\")}${toHttpPath(path)}`;\n    const serialized = this.serializer.serialize(input);\n    if (expectedMethod === \"GET\" && !(serialized instanceof FormData) && !isAsyncIteratorObject(serialized)) {\n      const maxUrlLength = await value(this.maxUrlLength, options, path, input);\n      const getUrl = new URL(url);\n      getUrl.searchParams.append(\"data\", stringifyJSON(serialized));\n      if (getUrl.toString().length <= maxUrlLength) {\n        return {\n          body: void 0,\n          method: expectedMethod,\n          headers,\n          url: getUrl,\n          signal: options.signal\n        };\n      }\n    }\n    return {\n      url,\n      method: expectedMethod === \"GET\" ? this.fallbackMethod : expectedMethod,\n      headers,\n      body: serialized,\n      signal: options.signal\n    };\n  }\n  async decode(response) {\n    const isOk = !isORPCErrorStatus(response.status);\n    const deserialized = await (async () => {\n      let isBodyOk = false;\n      try {\n        const body = await response.body();\n        isBodyOk = true;\n        return this.serializer.deserialize(body);\n      } catch (error) {\n        if (!isBodyOk) {\n          throw new Error(\"Cannot parse response body, please check the response body and content-type.\", {\n            cause: error\n          });\n        }\n        throw new Error(\"Invalid RPC response format.\", {\n          cause: error\n        });\n      }\n    })();\n    if (!isOk) {\n      if (isORPCErrorJson(deserialized)) {\n        throw createORPCErrorFromJson(deserialized);\n      }\n      throw new ORPCError(getMalformedResponseErrorCode(response.status), {\n        status: response.status,\n        data: { ...response, body: deserialized }\n      });\n    }\n    return deserialized;\n  }\n}\n\nclass StandardRPCSerializer {\n  constructor(jsonSerializer) {\n    this.jsonSerializer = jsonSerializer;\n  }\n  serialize(data) {\n    if (isAsyncIteratorObject(data)) {\n      return mapEventIterator(data, {\n        value: async (value) => this.#serialize(value, false),\n        error: async (e) => {\n          return new ErrorEvent({\n            data: this.#serialize(toORPCError(e).toJSON(), false),\n            cause: e\n          });\n        }\n      });\n    }\n    return this.#serialize(data, true);\n  }\n  #serialize(data, enableFormData) {\n    const [json, meta_, maps, blobs] = this.jsonSerializer.serialize(data);\n    const meta = meta_.length === 0 ? void 0 : meta_;\n    if (!enableFormData || blobs.length === 0) {\n      return {\n        json,\n        meta\n      };\n    }\n    const form = new FormData();\n    form.set(\"data\", stringifyJSON({ json, meta, maps }));\n    blobs.forEach((blob, i) => {\n      form.set(i.toString(), blob);\n    });\n    return form;\n  }\n  deserialize(data) {\n    if (isAsyncIteratorObject(data)) {\n      return mapEventIterator(data, {\n        value: async (value) => this.#deserialize(value),\n        error: async (e) => {\n          if (!(e instanceof ErrorEvent)) {\n            return e;\n          }\n          const deserialized = this.#deserialize(e.data);\n          if (isORPCErrorJson(deserialized)) {\n            return createORPCErrorFromJson(deserialized, { cause: e });\n          }\n          return new ErrorEvent({\n            data: deserialized,\n            cause: e\n          });\n        }\n      });\n    }\n    return this.#deserialize(data);\n  }\n  #deserialize(data) {\n    if (data === void 0) {\n      return void 0;\n    }\n    if (!(data instanceof FormData)) {\n      return this.jsonSerializer.deserialize(data.json, data.meta ?? []);\n    }\n    const serialized = JSON.parse(data.get(\"data\"));\n    return this.jsonSerializer.deserialize(\n      serialized.json,\n      serialized.meta ?? [],\n      serialized.maps,\n      (i) => data.get(i.toString())\n    );\n  }\n}\n\nclass StandardRPCLink extends StandardLink {\n  constructor(linkClient, options) {\n    const jsonSerializer = new StandardRPCJsonSerializer(options);\n    const serializer = new StandardRPCSerializer(jsonSerializer);\n    const linkCodec = new StandardRPCLinkCodec(serializer, options);\n    super(linkCodec, linkClient, options);\n  }\n}\n\nexport { CompositeStandardLinkPlugin as C, StandardLink as S, STANDARD_RPC_JSON_SERIALIZER_BUILT_IN_TYPES as a, StandardRPCJsonSerializer as b, StandardRPCLink as c, StandardRPCLinkCodec as d, StandardRPCSerializer as e, toStandardHeaders as f, getMalformedResponseErrorCode as g, toHttpPath as t };\n","import { StandardRPCJsonSerializer } from '@orpc/client/standard';\nimport { fallback, stringifyJSON, once } from '@orpc/shared';\nimport { getEventMeta, withEventMeta } from '@orpc/standard-server';\nimport { P as Publisher } from '../shared/experimental-publisher.BtlOkhPO.mjs';\n\nclass IORedisPublisher extends Publisher {\n  commander;\n  listener;\n  prefix;\n  serializer;\n  retentionSeconds;\n  subscriptionPromiseMap = /* @__PURE__ */ new Map();\n  listenersMap = /* @__PURE__ */ new Map();\n  onErrorsMap = /* @__PURE__ */ new Map();\n  redisListenerAndOnError;\n  get isResumeEnabled() {\n    return Number.isFinite(this.retentionSeconds) && this.retentionSeconds > 0;\n  }\n  /**\n   * The exactness of the `XTRIM` command.\n   *\n   * @internal\n   */\n  xtrimExactness = \"~\";\n  /**\n   * Useful for measuring memory usage.\n   *\n   * @internal\n   *\n   */\n  get size() {\n    let size = this.redisListenerAndOnError ? 1 : 0;\n    for (const listeners of this.listenersMap) {\n      size += listeners[1].length || 1;\n    }\n    for (const onErrors of this.onErrorsMap) {\n      size += onErrors[1].length || 1;\n    }\n    return size;\n  }\n  constructor({ commander, listener, resumeRetentionSeconds, prefix, ...options }) {\n    super(options);\n    this.commander = commander;\n    this.listener = listener;\n    this.prefix = fallback(prefix, \"orpc:publisher:\");\n    this.retentionSeconds = resumeRetentionSeconds ?? Number.NaN;\n    this.serializer = new StandardRPCJsonSerializer(options);\n  }\n  lastCleanupTimeMap = /* @__PURE__ */ new Map();\n  async publish(event, payload) {\n    const key = this.prefixKey(event);\n    const serialized = this.serializePayload(payload);\n    let id;\n    if (this.isResumeEnabled) {\n      const now = Date.now();\n      for (const [mapKey, lastCleanupTime] of this.lastCleanupTimeMap) {\n        if (lastCleanupTime + this.retentionSeconds * 1e3 < now) {\n          this.lastCleanupTimeMap.delete(mapKey);\n        }\n      }\n      if (!this.lastCleanupTimeMap.has(key)) {\n        this.lastCleanupTimeMap.set(key, now);\n        const result = await this.commander.multi().xadd(key, \"*\", \"data\", stringifyJSON(serialized)).xtrim(key, \"MINID\", this.xtrimExactness, `${now - this.retentionSeconds * 1e3}-0`).expire(key, this.retentionSeconds * 2).exec();\n        if (result) {\n          for (const [error] of result) {\n            if (error) {\n              throw error;\n            }\n          }\n        }\n        id = result[0][1];\n      } else {\n        const result = await this.commander.xadd(key, \"*\", \"data\", stringifyJSON(serialized));\n        id = result;\n      }\n    }\n    await this.commander.publish(key, stringifyJSON({ ...serialized, id }));\n  }\n  async subscribeListener(event, originalListener, { lastEventId, onError } = {}) {\n    const key = this.prefixKey(event);\n    let pendingPayloads = [];\n    const resumePayloadIds = /* @__PURE__ */ new Set();\n    const listener = (payload) => {\n      if (pendingPayloads) {\n        pendingPayloads.push(payload);\n        return;\n      }\n      const payloadId = getEventMeta(payload)?.id;\n      if (payloadId !== void 0 && resumePayloadIds.has(payloadId)) {\n        return;\n      }\n      originalListener(payload);\n    };\n    if (!this.redisListenerAndOnError) {\n      const redisOnError = (error) => {\n        for (const [_, onErrors] of this.onErrorsMap) {\n          for (const onError2 of onErrors) {\n            onError2(error);\n          }\n        }\n      };\n      const redisListener = (channel, message) => {\n        try {\n          const listeners2 = this.listenersMap.get(channel);\n          if (listeners2) {\n            const { id, ...rest } = JSON.parse(message);\n            const payload = this.deserializePayload(id, rest);\n            for (const listener2 of listeners2) {\n              listener2(payload);\n            }\n          }\n        } catch (error) {\n          const onErrors = this.onErrorsMap.get(channel);\n          if (onErrors) {\n            for (const onError2 of onErrors) {\n              onError2(error);\n            }\n          }\n        }\n      };\n      this.redisListenerAndOnError = { listener: redisListener, onError: redisOnError };\n      this.listener.on(\"message\", redisListener);\n      this.listener.on(\"error\", redisOnError);\n    }\n    const subscriptionPromise = this.subscriptionPromiseMap.get(key);\n    if (subscriptionPromise) {\n      await subscriptionPromise;\n    }\n    let listeners = this.listenersMap.get(key);\n    if (!listeners) {\n      try {\n        const promise = this.listener.subscribe(key);\n        this.subscriptionPromiseMap.set(key, promise);\n        await promise;\n        this.listenersMap.set(key, listeners = []);\n      } finally {\n        this.subscriptionPromiseMap.delete(key);\n        if (this.listenersMap.size === 0) {\n          this.listener.off(\"message\", this.redisListenerAndOnError.listener);\n          this.listener.off(\"error\", this.redisListenerAndOnError.onError);\n          this.redisListenerAndOnError = void 0;\n        }\n      }\n    }\n    listeners.push(listener);\n    if (onError) {\n      let onErrors = this.onErrorsMap.get(key);\n      if (!onErrors) {\n        this.onErrorsMap.set(key, onErrors = []);\n      }\n      onErrors.push(onError);\n    }\n    void (async () => {\n      try {\n        if (this.isResumeEnabled && typeof lastEventId === \"string\") {\n          const results = await this.commander.xread(\"STREAMS\", key, lastEventId);\n          if (results && results[0]) {\n            const [_, items] = results[0];\n            for (const [id, fields] of items) {\n              const serialized = fields[1];\n              const payload = this.deserializePayload(id, JSON.parse(serialized));\n              resumePayloadIds.add(id);\n              originalListener(payload);\n            }\n          }\n        }\n      } catch (error) {\n        onError?.(error);\n      } finally {\n        const pending = pendingPayloads;\n        pendingPayloads = void 0;\n        for (const payload of pending) {\n          listener(payload);\n        }\n      }\n    })();\n    const cleanupListeners = once(() => {\n      listeners.splice(listeners.indexOf(listener), 1);\n      if (onError) {\n        const onErrors = this.onErrorsMap.get(key);\n        if (onErrors) {\n          const index = onErrors.indexOf(onError);\n          if (index !== -1) {\n            onErrors.splice(index, 1);\n          }\n        }\n      }\n    });\n    return async () => {\n      cleanupListeners();\n      if (listeners.length === 0) {\n        this.listenersMap.delete(key);\n        this.onErrorsMap.delete(key);\n        if (this.redisListenerAndOnError && this.listenersMap.size === 0) {\n          this.listener.off(\"message\", this.redisListenerAndOnError.listener);\n          this.listener.off(\"error\", this.redisListenerAndOnError.onError);\n          this.redisListenerAndOnError = void 0;\n        }\n        await this.listener.unsubscribe(key);\n      }\n    };\n  }\n  prefixKey(key) {\n    return `${this.prefix}${key}`;\n  }\n  serializePayload(payload) {\n    const eventMeta = getEventMeta(payload);\n    const [json, meta] = this.serializer.serialize(payload);\n    return { json, meta, eventMeta };\n  }\n  deserializePayload(id, { json, meta, eventMeta }) {\n    return withEventMeta(\n      this.serializer.deserialize(json, meta),\n      id === void 0 ? { ...eventMeta } : { ...eventMeta, id }\n    );\n  }\n}\n\nexport { IORedisPublisher };\n","import { EventPublisher, SequentialIdGenerator, compareSequentialIds } from '@orpc/shared';\nimport { withEventMeta, getEventMeta } from '@orpc/standard-server';\nimport { P as Publisher } from '../shared/experimental-publisher.BtlOkhPO.mjs';\n\nclass MemoryPublisher extends Publisher {\n  eventPublisher = new EventPublisher();\n  idGenerator = new SequentialIdGenerator();\n  retentionSeconds;\n  eventsMap = /* @__PURE__ */ new Map();\n  /**\n   * Useful for measuring memory usage.\n   *\n   * @internal\n   *\n   */\n  get size() {\n    let size = this.eventPublisher.size;\n    for (const events of this.eventsMap) {\n      size += events[1].length || 1;\n    }\n    return size;\n  }\n  get isResumeEnabled() {\n    return Number.isFinite(this.retentionSeconds) && this.retentionSeconds > 0;\n  }\n  constructor({ resumeRetentionSeconds, ...options } = {}) {\n    super(options);\n    this.retentionSeconds = resumeRetentionSeconds ?? Number.NaN;\n  }\n  async publish(event, payload) {\n    this.cleanup();\n    if (this.isResumeEnabled) {\n      const now = Date.now();\n      const expiresAt = now + this.retentionSeconds * 1e3;\n      let events = this.eventsMap.get(event);\n      if (!events) {\n        this.eventsMap.set(event, events = []);\n      }\n      payload = withEventMeta(payload, { ...getEventMeta(payload), id: this.idGenerator.generate() });\n      events.push({ expiresAt, payload });\n    }\n    this.eventPublisher.publish(event, payload);\n  }\n  async subscribeListener(event, listener, options) {\n    if (this.isResumeEnabled && typeof options?.lastEventId === \"string\") {\n      const events = this.eventsMap.get(event);\n      if (events) {\n        for (const { payload } of events) {\n          const id = getEventMeta(payload)?.id;\n          if (typeof id === \"string\" && compareSequentialIds(id, options.lastEventId) > 0) {\n            listener(payload);\n          }\n        }\n      }\n    }\n    const syncUnsub = this.eventPublisher.subscribe(event, listener);\n    return async () => {\n      syncUnsub();\n    };\n  }\n  lastCleanupTime = null;\n  cleanup() {\n    if (!this.isResumeEnabled) {\n      return;\n    }\n    const now = Date.now();\n    if (this.lastCleanupTime !== null && this.lastCleanupTime + this.retentionSeconds * 1e3 > now) {\n      return;\n    }\n    this.lastCleanupTime = now;\n    for (const [event, events] of this.eventsMap) {\n      const validEvents = events.filter((event2) => event2.expiresAt > now);\n      if (validEvents.length > 0) {\n        this.eventsMap.set(event, validEvents);\n      } else {\n        this.eventsMap.delete(event);\n      }\n    }\n  }\n}\n\nexport { MemoryPublisher };\n","import { StandardRPCJsonSerializer } from '@orpc/client/standard';\nimport { fallback, once } from '@orpc/shared';\nimport { getEventMeta, withEventMeta } from '@orpc/standard-server';\nimport { P as Publisher } from '../shared/experimental-publisher.BtlOkhPO.mjs';\n\nclass UpstashRedisPublisher extends Publisher {\n  constructor(redis, { resumeRetentionSeconds, prefix, ...options } = {}) {\n    super(options);\n    this.redis = redis;\n    this.prefix = fallback(prefix, \"orpc:publisher:\");\n    this.retentionSeconds = resumeRetentionSeconds ?? Number.NaN;\n    this.serializer = new StandardRPCJsonSerializer(options);\n  }\n  prefix;\n  serializer;\n  retentionSeconds;\n  listenersMap = /* @__PURE__ */ new Map();\n  onErrorsMap = /* @__PURE__ */ new Map();\n  subscriptionPromiseMap = /* @__PURE__ */ new Map();\n  subscriptionsMap = /* @__PURE__ */ new Map();\n  // Upstash subscription objects\n  get isResumeEnabled() {\n    return Number.isFinite(this.retentionSeconds) && this.retentionSeconds > 0;\n  }\n  /**\n   * The exactness of the `XTRIM` command.\n   *\n   * @internal\n   */\n  xtrimExactness = \"~\";\n  /**\n   * Useful for measuring memory usage.\n   *\n   * @internal\n   *\n   */\n  get size() {\n    let size = 0;\n    for (const listeners of this.listenersMap) {\n      size += listeners[1].length || 1;\n    }\n    for (const onErrors of this.onErrorsMap) {\n      size += onErrors[1].length || 1;\n    }\n    return size;\n  }\n  lastCleanupTimeMap = /* @__PURE__ */ new Map();\n  async publish(event, payload) {\n    const key = this.prefixKey(event);\n    const serialized = this.serializePayload(payload);\n    let id;\n    if (this.isResumeEnabled) {\n      const now = Date.now();\n      for (const [mapKey, lastCleanupTime] of this.lastCleanupTimeMap) {\n        if (lastCleanupTime + this.retentionSeconds * 1e3 < now) {\n          this.lastCleanupTimeMap.delete(mapKey);\n        }\n      }\n      if (!this.lastCleanupTimeMap.has(key)) {\n        this.lastCleanupTimeMap.set(key, now);\n        const results = await this.redis.multi().xadd(key, \"*\", { data: serialized }).xtrim(key, { strategy: \"MINID\", exactness: this.xtrimExactness, threshold: `${now - this.retentionSeconds * 1e3}-0` }).expire(key, this.retentionSeconds * 2).exec();\n        id = results[0];\n      } else {\n        const result = await this.redis.xadd(key, \"*\", { data: serialized });\n        id = result;\n      }\n    }\n    await this.redis.publish(key, { ...serialized, id });\n  }\n  async subscribeListener(event, originalListener, { lastEventId, onError } = {}) {\n    const key = this.prefixKey(event);\n    let pendingPayloads = [];\n    const resumePayloadIds = /* @__PURE__ */ new Set();\n    const listener = (payload) => {\n      if (pendingPayloads) {\n        pendingPayloads.push(payload);\n        return;\n      }\n      const payloadId = getEventMeta(payload)?.id;\n      if (payloadId !== void 0 && resumePayloadIds.has(payloadId)) {\n        return;\n      }\n      originalListener(payload);\n    };\n    const subscriptionPromise = this.subscriptionPromiseMap.get(key);\n    if (subscriptionPromise) {\n      await subscriptionPromise;\n    }\n    let subscription = this.subscriptionsMap.get(key);\n    if (!subscription) {\n      const dispatchErrorForKey = (error) => {\n        const onErrors = this.onErrorsMap.get(key);\n        if (onErrors) {\n          for (const onError2 of onErrors) {\n            onError2(error);\n          }\n        }\n      };\n      subscription = this.redis.subscribe(key);\n      subscription.on(\"message\", (event2) => {\n        try {\n          const listeners2 = this.listenersMap.get(event2.channel);\n          if (listeners2) {\n            const { id, ...rest } = event2.message;\n            const payload = this.deserializePayload(id, rest);\n            for (const listener2 of listeners2) {\n              listener2(payload);\n            }\n          }\n        } catch (error) {\n          dispatchErrorForKey(error);\n        }\n      });\n      let resolvePromise;\n      let rejectPromise;\n      const promise = new Promise((resolve, reject) => {\n        resolvePromise = resolve;\n        rejectPromise = reject;\n      });\n      subscription.on(\"error\", (error) => {\n        rejectPromise(error);\n        dispatchErrorForKey(error);\n      });\n      subscription.on(\"subscribe\", () => {\n        resolvePromise();\n      });\n      try {\n        this.subscriptionPromiseMap.set(key, promise);\n        await promise;\n        this.subscriptionsMap.set(key, subscription);\n      } finally {\n        this.subscriptionPromiseMap.delete(key);\n      }\n    }\n    let listeners = this.listenersMap.get(key);\n    if (!listeners) {\n      this.listenersMap.set(key, listeners = []);\n    }\n    listeners.push(listener);\n    if (onError) {\n      let onErrors = this.onErrorsMap.get(key);\n      if (!onErrors) {\n        this.onErrorsMap.set(key, onErrors = []);\n      }\n      onErrors.push(onError);\n    }\n    void (async () => {\n      try {\n        if (this.isResumeEnabled && typeof lastEventId === \"string\") {\n          const results = await this.redis.xread(key, lastEventId);\n          if (results && results[0]) {\n            const [_, items] = results[0];\n            for (const [id, fields] of items) {\n              const serialized = fields[1];\n              const payload = this.deserializePayload(id, serialized);\n              resumePayloadIds.add(id);\n              originalListener(payload);\n            }\n          }\n        }\n      } catch (error) {\n        onError?.(error);\n      } finally {\n        const pending = pendingPayloads;\n        pendingPayloads = void 0;\n        for (const payload of pending) {\n          listener(payload);\n        }\n      }\n    })();\n    const cleanupListeners = once(() => {\n      listeners.splice(listeners.indexOf(listener), 1);\n      if (onError) {\n        const onErrors = this.onErrorsMap.get(key);\n        if (onErrors) {\n          onErrors.splice(onErrors.indexOf(onError), 1);\n        }\n      }\n    });\n    return async () => {\n      cleanupListeners();\n      if (listeners.length === 0) {\n        this.listenersMap.delete(key);\n        this.onErrorsMap.delete(key);\n        const subscription2 = this.subscriptionsMap.get(key);\n        if (subscription2) {\n          this.subscriptionsMap.delete(key);\n          await subscription2.unsubscribe();\n        }\n      }\n    };\n  }\n  prefixKey(key) {\n    return `${this.prefix}${key}`;\n  }\n  serializePayload(payload) {\n    const eventMeta = getEventMeta(payload);\n    const [json, meta] = this.serializer.serialize(payload);\n    return { json, meta, eventMeta };\n  }\n  deserializePayload(id, { json, meta, eventMeta }) {\n    return withEventMeta(\n      this.serializer.deserialize(json, meta),\n      id === void 0 ? { ...eventMeta } : { ...eventMeta, id }\n    );\n  }\n}\n\nexport { UpstashRedisPublisher };\n","export { P as Publisher } from './shared/experimental-publisher.BtlOkhPO.mjs';\nimport '@orpc/shared';\n","import { AsyncIteratorClass } from '@orpc/shared';\n\nclass Publisher {\n  maxBufferedEvents;\n  constructor(options = {}) {\n    this.maxBufferedEvents = options.maxBufferedEvents ?? 100;\n  }\n  subscribe(event, listenerOrOptions, listenerOptions) {\n    if (typeof listenerOrOptions === \"function\") {\n      return this.subscribeListener(event, listenerOrOptions, listenerOptions);\n    }\n    const signal = listenerOrOptions?.signal;\n    const maxBufferedEvents = listenerOrOptions?.maxBufferedEvents ?? this.maxBufferedEvents;\n    signal?.throwIfAborted();\n    const bufferedEvents = [];\n    const pullResolvers = [];\n    let subscriptionError;\n    const unsubscribePromise = this.subscribe(event, (payload) => {\n      const resolver = pullResolvers.shift();\n      if (resolver) {\n        resolver[0]({ done: false, value: payload });\n      } else {\n        bufferedEvents.push(payload);\n        if (bufferedEvents.length > maxBufferedEvents) {\n          bufferedEvents.shift();\n        }\n      }\n    }, {\n      lastEventId: listenerOrOptions?.lastEventId,\n      onError: (error) => {\n        subscriptionError = { error };\n        pullResolvers.forEach((resolver) => resolver[1](error));\n        signal?.removeEventListener(\"abort\", abortListener);\n        pullResolvers.length = 0;\n        bufferedEvents.length = 0;\n        unsubscribePromise.then((unsubscribe) => unsubscribe()).catch(() => {\n        });\n      }\n    });\n    function abortListener(event2) {\n      pullResolvers.forEach((resolver) => resolver[1](event2.target.reason));\n      pullResolvers.length = 0;\n      bufferedEvents.length = 0;\n      unsubscribePromise.then((unsubscribe) => unsubscribe()).catch(() => {\n      });\n    }\n    signal?.addEventListener(\"abort\", abortListener, { once: true });\n    return new AsyncIteratorClass(async () => {\n      if (subscriptionError) {\n        throw subscriptionError.error;\n      }\n      if (signal?.aborted) {\n        throw signal.reason;\n      }\n      await unsubscribePromise;\n      if (bufferedEvents.length > 0) {\n        return { done: false, value: bufferedEvents.shift() };\n      }\n      return new Promise((resolve, reject) => {\n        pullResolvers.push([resolve, reject]);\n      });\n    }, async () => {\n      pullResolvers.forEach((resolver) => resolver[0]({ done: true, value: void 0 }));\n      signal?.removeEventListener(\"abort\", abortListener);\n      pullResolvers.length = 0;\n      bufferedEvents.length = 0;\n      await unsubscribePromise.then((unsubscribe) => unsubscribe());\n    });\n  }\n}\n\nexport { Publisher as P };\n","import { AsyncIteratorClass, startSpan, runInSpanContext, AbortError, parseEmptyableJSON, isTypescriptObject, setSpanError, stringifyJSON, runWithSpan, isAsyncIteratorObject, once } from '@orpc/shared';\nimport { EventDecoderStream, withEventMeta, ErrorEvent, encodeEventMessage, getEventMeta, getFilenameFromContentDisposition, generateContentDisposition } from '@orpc/standard-server';\n\nfunction toEventIterator(stream, options = {}) {\n  const eventStream = stream?.pipeThrough(new TextDecoderStream()).pipeThrough(new EventDecoderStream());\n  const reader = eventStream?.getReader();\n  let span;\n  let isCancelled = false;\n  return new AsyncIteratorClass(async () => {\n    span ??= startSpan(\"consume_event_iterator_stream\");\n    try {\n      while (true) {\n        if (reader === void 0) {\n          return { done: true, value: void 0 };\n        }\n        const { done, value } = await runInSpanContext(span, () => reader.read());\n        if (done) {\n          if (isCancelled) {\n            throw new AbortError(\"Stream was cancelled\");\n          }\n          return { done: true, value: void 0 };\n        }\n        switch (value.event) {\n          case \"message\": {\n            let message = parseEmptyableJSON(value.data);\n            if (isTypescriptObject(message)) {\n              message = withEventMeta(message, value);\n            }\n            span?.addEvent(\"message\");\n            return { done: false, value: message };\n          }\n          case \"error\": {\n            let error = new ErrorEvent({\n              data: parseEmptyableJSON(value.data)\n            });\n            error = withEventMeta(error, value);\n            span?.addEvent(\"error\");\n            throw error;\n          }\n          case \"done\": {\n            let done2 = parseEmptyableJSON(value.data);\n            if (isTypescriptObject(done2)) {\n              done2 = withEventMeta(done2, value);\n            }\n            span?.addEvent(\"done\");\n            return { done: true, value: done2 };\n          }\n          default: {\n            span?.addEvent(\"maybe_keepalive\");\n          }\n        }\n      }\n    } catch (e) {\n      if (!(e instanceof ErrorEvent)) {\n        setSpanError(span, e, options);\n      }\n      throw e;\n    }\n  }, async (reason) => {\n    try {\n      if (reason !== \"next\") {\n        isCancelled = true;\n        span?.addEvent(\"cancelled\");\n      }\n      await runInSpanContext(span, () => reader?.cancel());\n    } catch (e) {\n      setSpanError(span, e, options);\n      throw e;\n    } finally {\n      span?.end();\n    }\n  });\n}\nfunction toEventStream(iterator, options = {}) {\n  const keepAliveEnabled = options.eventIteratorKeepAliveEnabled ?? true;\n  const keepAliveInterval = options.eventIteratorKeepAliveInterval ?? 5e3;\n  const keepAliveComment = options.eventIteratorKeepAliveComment ?? \"\";\n  let cancelled = false;\n  let timeout;\n  let span;\n  const stream = new ReadableStream({\n    start() {\n      span = startSpan(\"stream_event_iterator\");\n    },\n    async pull(controller) {\n      try {\n        if (keepAliveEnabled) {\n          timeout = setInterval(() => {\n            controller.enqueue(encodeEventMessage({\n              comments: [keepAliveComment]\n            }));\n            span?.addEvent(\"keepalive\");\n          }, keepAliveInterval);\n        }\n        const value = await runInSpanContext(span, () => iterator.next());\n        clearInterval(timeout);\n        if (cancelled) {\n          return;\n        }\n        const meta = getEventMeta(value.value);\n        if (!value.done || value.value !== void 0 || meta !== void 0) {\n          const event = value.done ? \"done\" : \"message\";\n          controller.enqueue(encodeEventMessage({\n            ...meta,\n            event,\n            data: stringifyJSON(value.value)\n          }));\n          span?.addEvent(event);\n        }\n        if (value.done) {\n          controller.close();\n          span?.end();\n        }\n      } catch (err) {\n        clearInterval(timeout);\n        if (cancelled) {\n          return;\n        }\n        if (err instanceof ErrorEvent) {\n          controller.enqueue(encodeEventMessage({\n            ...getEventMeta(err),\n            event: \"error\",\n            data: stringifyJSON(err.data)\n          }));\n          span?.addEvent(\"error\");\n          controller.close();\n        } else {\n          setSpanError(span, err);\n          controller.error(err);\n        }\n        span?.end();\n      }\n    },\n    async cancel() {\n      try {\n        cancelled = true;\n        clearInterval(timeout);\n        span?.addEvent(\"cancelled\");\n        await runInSpanContext(span, () => iterator.return?.());\n      } catch (e) {\n        setSpanError(span, e);\n        throw e;\n      } finally {\n        span?.end();\n      }\n    }\n  }).pipeThrough(new TextEncoderStream());\n  return stream;\n}\n\nfunction toStandardBody(re, options = {}) {\n  return runWithSpan(\n    { name: \"parse_standard_body\", signal: options.signal },\n    async () => {\n      const contentDisposition = re.headers.get(\"content-disposition\");\n      if (typeof contentDisposition === \"string\") {\n        const fileName = getFilenameFromContentDisposition(contentDisposition) ?? \"blob\";\n        const blob2 = await re.blob();\n        return new File([blob2], fileName, {\n          type: blob2.type\n        });\n      }\n      const contentType = re.headers.get(\"content-type\");\n      if (!contentType || contentType.startsWith(\"application/json\")) {\n        const text = await re.text();\n        return parseEmptyableJSON(text);\n      }\n      if (contentType.startsWith(\"multipart/form-data\")) {\n        return await re.formData();\n      }\n      if (contentType.startsWith(\"application/x-www-form-urlencoded\")) {\n        const text = await re.text();\n        return new URLSearchParams(text);\n      }\n      if (contentType.startsWith(\"text/event-stream\")) {\n        return toEventIterator(re.body, options);\n      }\n      if (contentType.startsWith(\"text/plain\")) {\n        return await re.text();\n      }\n      const blob = await re.blob();\n      return new File([blob], \"blob\", {\n        type: blob.type\n      });\n    }\n  );\n}\nfunction toFetchBody(body, headers, options = {}) {\n  const currentContentDisposition = headers.get(\"content-disposition\");\n  headers.delete(\"content-type\");\n  headers.delete(\"content-disposition\");\n  if (body === void 0) {\n    return void 0;\n  }\n  if (body instanceof Blob) {\n    headers.set(\"content-type\", body.type);\n    headers.set(\"content-length\", body.size.toString());\n    headers.set(\n      \"content-disposition\",\n      currentContentDisposition ?? generateContentDisposition(body instanceof File ? body.name : \"blob\")\n    );\n    return body;\n  }\n  if (body instanceof FormData) {\n    return body;\n  }\n  if (body instanceof URLSearchParams) {\n    return body;\n  }\n  if (isAsyncIteratorObject(body)) {\n    headers.set(\"content-type\", \"text/event-stream\");\n    return toEventStream(body, options);\n  }\n  headers.set(\"content-type\", \"application/json\");\n  return stringifyJSON(body);\n}\n\nfunction toStandardHeaders(headers, standardHeaders = {}) {\n  headers.forEach((value, key) => {\n    if (Array.isArray(standardHeaders[key])) {\n      standardHeaders[key].push(value);\n    } else if (standardHeaders[key] !== void 0) {\n      standardHeaders[key] = [standardHeaders[key], value];\n    } else {\n      standardHeaders[key] = value;\n    }\n  });\n  return standardHeaders;\n}\nfunction toFetchHeaders(headers, fetchHeaders = new Headers()) {\n  for (const [key, value] of Object.entries(headers)) {\n    if (Array.isArray(value)) {\n      for (const v of value) {\n        fetchHeaders.append(key, v);\n      }\n    } else if (value !== void 0) {\n      fetchHeaders.append(key, value);\n    }\n  }\n  return fetchHeaders;\n}\n\nfunction toStandardLazyRequest(request) {\n  return {\n    url: new URL(request.url),\n    signal: request.signal,\n    method: request.method,\n    body: once(() => toStandardBody(request, { signal: request.signal })),\n    get headers() {\n      const headers = toStandardHeaders(request.headers);\n      Object.defineProperty(this, \"headers\", { value: headers, writable: true });\n      return headers;\n    },\n    set headers(value) {\n      Object.defineProperty(this, \"headers\", { value, writable: true });\n    }\n  };\n}\nfunction toFetchRequest(request, options = {}) {\n  const headers = toFetchHeaders(request.headers);\n  const body = toFetchBody(request.body, headers, options);\n  return new Request(request.url, {\n    signal: request.signal,\n    method: request.method,\n    headers,\n    body\n  });\n}\n\nfunction toFetchResponse(response, options = {}) {\n  const headers = toFetchHeaders(response.headers);\n  const body = toFetchBody(response.body, headers, options);\n  return new Response(body, { headers, status: response.status });\n}\nfunction toStandardLazyResponse(response, options = {}) {\n  return {\n    body: once(() => toStandardBody(response, options)),\n    status: response.status,\n    get headers() {\n      const headers = toStandardHeaders(response.headers);\n      Object.defineProperty(this, \"headers\", { value: headers, writable: true });\n      return headers;\n    },\n    set headers(value) {\n      Object.defineProperty(this, \"headers\", { value, writable: true });\n    }\n  };\n}\n\nexport { toEventIterator, toEventStream, toFetchBody, toFetchHeaders, toFetchRequest, toFetchResponse, toStandardBody, toStandardHeaders, toStandardLazyRequest, toStandardLazyResponse };\n","import { createRequire } from \"node:module\";\n\n//#region rolldown:runtime\nvar __create = Object.create;\nvar __defProp = Object.defineProperty;\nvar __getOwnPropDesc = Object.getOwnPropertyDescriptor;\nvar __getOwnPropNames = Object.getOwnPropertyNames;\nvar __getProtoOf = Object.getPrototypeOf;\nvar __hasOwnProp = Object.prototype.hasOwnProperty;\nvar __commonJS = (cb, mod) => function() {\n\treturn mod || (0, cb[__getOwnPropNames(cb)[0]])((mod = { exports: {} }).exports, mod), mod.exports;\n};\nvar __copyProps = (to, from, except, desc) => {\n\tif (from && typeof from === \"object\" || typeof from === \"function\") for (var keys = __getOwnPropNames(from), i = 0, n = keys.length, key; i < n; i++) {\n\t\tkey = keys[i];\n\t\tif (!__hasOwnProp.call(to, key) && key !== except) __defProp(to, key, {\n\t\t\tget: ((k) => from[k]).bind(null, key),\n\t\t\tenumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable\n\t\t});\n\t}\n\treturn to;\n};\nvar __reExport = (target, mod, secondTarget) => (__copyProps(target, mod, \"default\"), secondTarget && __copyProps(secondTarget, mod, \"default\"));\nvar __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(isNodeMode || !mod || !mod.__esModule ? __defProp(target, \"default\", {\n\tvalue: mod,\n\tenumerable: true\n}) : target, mod));\nvar __require = /* @__PURE__ */ createRequire(import.meta.url);\n\n//#endregion\nexport { __toESM as i, __reExport as n, __require as r, __commonJS as t };","export * from \"effect\"\n\nexport {  };","import { n as __reExport } from \"./chunk-QQAxy9l9.mjs\";\n\n//#region src/zod.ts\nvar zod_exports = {};\nimport * as import_zod from \"zod\";\n__reExport(zod_exports, import_zod);\n\n//#endregion\nexport { zod_exports as t };\n//# sourceMappingURL=data:application/json;charset=utf-8;base64,eyJ2ZXJzaW9uIjozLCJmaWxlIjoiem9kLUJWbzUxa2dKLm1qcyIsIm5hbWVzIjpbXSwic291cmNlcyI6WyIuLi9zcmMvem9kLnRzIl0sInNvdXJjZXNDb250ZW50IjpbImV4cG9ydCAqIGZyb20gXCJ6b2RcIjtcbiJdLCJtYXBwaW5ncyI6IiJ9","import \"./zod-BVo51kgJ.mjs\";\n\nexport * from \"zod\"\n\nexport {  };","import { createRequire } from \"node:module\";\n\n//#region rolldown:runtime\nvar __create = Object.create;\nvar __defProp = Object.defineProperty;\nvar __getOwnPropDesc = Object.getOwnPropertyDescriptor;\nvar __getOwnPropNames = Object.getOwnPropertyNames;\nvar __getProtoOf = Object.getPrototypeOf;\nvar __hasOwnProp = Object.prototype.hasOwnProperty;\nvar __commonJS = (cb, mod) => function() {\n\treturn mod || (0, cb[__getOwnPropNames(cb)[0]])((mod = { exports: {} }).exports, mod), mod.exports;\n};\nvar __copyProps = (to, from, except, desc) => {\n\tif (from && typeof from === \"object\" || typeof from === \"function\") for (var keys = __getOwnPropNames(from), i = 0, n = keys.length, key; i < n; i++) {\n\t\tkey = keys[i];\n\t\tif (!__hasOwnProp.call(to, key) && key !== except) __defProp(to, key, {\n\t\t\tget: ((k) => from[k]).bind(null, key),\n\t\t\tenumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable\n\t\t});\n\t}\n\treturn to;\n};\nvar __reExport = (target, mod, secondTarget) => (__copyProps(target, mod, \"default\"), secondTarget && __copyProps(secondTarget, mod, \"default\"));\nvar __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(isNodeMode || !mod || !mod.__esModule ? __defProp(target, \"default\", {\n\tvalue: mod,\n\tenumerable: true\n}) : target, mod));\nvar __require = /* @__PURE__ */ createRequire(import.meta.url);\n\n//#endregion\nexport { __toESM as i, __reExport as n, __require as r, __commonJS as t };","import { IORedisPublisher } from \"@orpc/experimental-publisher/ioredis\";\nimport { MemoryPublisher } from \"@orpc/experimental-publisher/memory\";\nimport { UpstashRedisPublisher } from \"@orpc/experimental-publisher/upstash-redis\";\n\nexport * from \"@orpc/server\"\n\nexport * from \"@orpc/contract\"\n\nexport * from \"@orpc/experimental-publisher\"\n\nexport { IORedisPublisher, MemoryPublisher, UpstashRedisPublisher };","import { n as __reExport } from \"./chunk-QQAxy9l9.mjs\";\n\n//#region src/zod.ts\nvar zod_exports = {};\nimport * as import_zod from \"zod\";\n__reExport(zod_exports, import_zod);\n\n//#endregion\nexport { zod_exports as t };\n//# sourceMappingURL=data:application/json;charset=utf-8;base64,eyJ2ZXJzaW9uIjozLCJmaWxlIjoiem9kLUJWbzUxa2dKLm1qcyIsIm5hbWVzIjpbXSwic291cmNlcyI6WyIuLi9zcmMvem9kLnRzIl0sInNvdXJjZXNDb250ZW50IjpbImV4cG9ydCAqIGZyb20gXCJ6b2RcIjtcbiJdLCJtYXBwaW5ncyI6IiJ9","import \"./zod-BVo51kgJ.mjs\";\n\nexport * from \"zod\"\n\nexport {  };"],"names":[],"mappings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jFA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AChNA;AACA;;;;;;;;;;;;;;ACDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;ACvEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;ACjSA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;AC7BA;AACA;;;;;;;;;;;;;;;;ACDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACTA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;ACHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AC7BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;ACTA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACTA;AACA;AACA;AACA"}